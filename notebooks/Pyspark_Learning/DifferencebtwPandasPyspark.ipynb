{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "251bf43a-dda0-4ac0-bdf4-8ea160f4ad0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas Execution Time: 0.0082 seconds\n",
      "Pandas Memory Usage: current=85.37 KB, peak=95.40 KB\n",
      "Pandas Result:\n",
      "customer_id\n",
      "1    250\n",
      "2    450\n",
      "3     50\n",
      "Name: transaction_amount, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import tracemalloc\n",
    "\n",
    "# Sample data (in-memory)\n",
    "data = {'customer_id': [1, 2, 1, 3, 2], \n",
    "        'transaction_amount': [100, 200, 150, 50, 250],\n",
    "        'transaction_date': ['2025-02-01', '2025-02-01', '2025-02-02', '2025-02-02', '2025-02-03']}\n",
    "df_pandas = pd.DataFrame(data)\n",
    "\n",
    "# Track memory usage and time\n",
    "tracemalloc.start()\n",
    "start_time = time.time()\n",
    "\n",
    "# Transformation: Group by customer_id and sum transaction_amount\n",
    "df_pandas_grouped = df_pandas.groupby('customer_id')['transaction_amount'].sum()\n",
    "\n",
    "# Stop tracking\n",
    "end_time = time.time()\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "\n",
    "print(f\"Pandas Execution Time: {end_time - start_time:.4f} seconds\")\n",
    "print(f\"Pandas Memory Usage: current={current / 1024:.2f} KB, peak={peak / 1024:.2f} KB\")\n",
    "print(f\"Pandas Result:\\n{df_pandas_grouped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2483b473-0373-407b-ad8c-330b2b5eeb99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark Execution Time: 2.2653 seconds\n",
      "PySpark Memory Usage: current=349.06 KB, peak=480.80 KB\n",
      "+-----------+------------+\n",
      "|customer_id|total_amount|\n",
      "+-----------+------------+\n",
      "|          1|         250|\n",
      "|          2|         450|\n",
      "|          3|          50|\n",
      "+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum\n",
    "import time\n",
    "import tracemalloc\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"PySparkExample\").getOrCreate()\n",
    "\n",
    "# Sample data (simulating distributed data)\n",
    "data = [{'customer_id': 1, 'transaction_amount': 100, 'transaction_date': '2025-02-01'},\n",
    "        {'customer_id': 2, 'transaction_amount': 200, 'transaction_date': '2025-02-01'},\n",
    "        {'customer_id': 1, 'transaction_amount': 150, 'transaction_date': '2025-02-02'},\n",
    "        {'customer_id': 3, 'transaction_amount': 50, 'transaction_date': '2025-02-02'},\n",
    "        {'customer_id': 2, 'transaction_amount': 250, 'transaction_date': '2025-02-03'}]\n",
    "df_spark = spark.createDataFrame(data)\n",
    "\n",
    "# Track memory usage and time\n",
    "tracemalloc.start()\n",
    "start_time = time.time()\n",
    "\n",
    "# Transformation: Group by customer_id and sum transaction_amount\n",
    "df_spark_grouped = df_spark.groupBy('customer_id').agg(sum('transaction_amount').alias('total_amount'))\n",
    "\n",
    "# Action to trigger computation and measure time correctly\n",
    "df_spark_grouped.collect()\n",
    "\n",
    "# Stop tracking\n",
    "end_time = time.time()\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "\n",
    "print(f\"PySpark Execution Time: {end_time - start_time:.4f} seconds\")\n",
    "print(f\"PySpark Memory Usage: current={current / 1024:.2f} KB, peak={peak / 1024:.2f} KB\")\n",
    "df_spark_grouped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe928136-1b79-4151-8b49-63c818d51d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas - Memory usage before filter: 323.96 MB\n",
      "Pandas - Memory usage after filter: 515.84 MB\n",
      "Pandas - Filter time: 0.2346 seconds\n",
      "Pandas - Memory usage before aggregation: 515.84 MB\n",
      "Pandas - Memory usage after aggregation: 521.59 MB\n",
      "Pandas - Aggregation time: 0.1287 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "# Generate sample data\n",
    "data = {'col1': range(10000000), 'col2': [x % 100 for x in range(10000000)]}\n",
    "df_pandas = pd.DataFrame(data)\n",
    "\n",
    "# Transformation 1: Filter\n",
    "start_time = time.time()\n",
    "process = psutil.Process()\n",
    "mem_before_filter = process.memory_info().rss / (1024 ** 2)\n",
    "df_filtered_pandas = df_pandas[df_pandas['col2'] > 50]\n",
    "mem_after_filter = process.memory_info().rss / (1024 ** 2)\n",
    "filter_time = time.time() - start_time\n",
    "\n",
    "# Transformation 2: Aggregate\n",
    "start_time = time.time()\n",
    "mem_before_agg = process.memory_info().rss / (1024 ** 2)\n",
    "df_agg_pandas = df_filtered_pandas.groupby('col2').mean()\n",
    "mem_after_agg = process.memory_info().rss / (1024 ** 2)\n",
    "agg_time = time.time() - start_time\n",
    "\n",
    "print(f\"Pandas - Memory usage before filter: {mem_before_filter:.2f} MB\")\n",
    "print(f\"Pandas - Memory usage after filter: {mem_after_filter:.2f} MB\")\n",
    "print(f\"Pandas - Filter time: {filter_time:.4f} seconds\")\n",
    "print(f\"Pandas - Memory usage before aggregation: {mem_before_agg:.2f} MB\")\n",
    "print(f\"Pandas - Memory usage after aggregation: {mem_after_agg:.2f} MB\")\n",
    "print(f\"Pandas - Aggregation time: {agg_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d302b04-70a1-4fc4-83da-ba5bc63855c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"PySparkExample\").getOrCreate()\n",
    "\n",
    "# Generate sample data\n",
    "data = list(zip(range(10000000), [x % 100 for x in range(10000000)]))\n",
    "df_spark = spark.createDataFrame(data, [\"col1\", \"col2\"])\n",
    "\n",
    "# Transformation 1: Filter\n",
    "start_time = time.time()\n",
    "process = psutil.Process()\n",
    "mem_before_filter = process.memory_info().rss / (1024 ** 2)\n",
    "df_filtered_spark = df_spark.filter(df_spark['col2'] > 50)\n",
    "df_filtered_spark.cache()  # Cache the result for subsequent operations\n",
    "df_filtered_spark.count()  # Trigger execution and materialize the cache\n",
    "mem_after_filter = process.memory_info().rss / (1024 ** 2)\n",
    "filter_time = time.time() - start_time\n",
    "\n",
    "# Transformation 2: Aggregate\n",
    "start_time = time.time()\n",
    "mem_before_agg = process.memory_info().rss / (1024 ** 2)\n",
    "df_agg_spark = df_filtered_spark.groupBy('col2').mean()\n",
    "df_agg_spark.count()  # Trigger execution\n",
    "mem_after_agg = process.memory_info().rss / (1024 ** 2)\n",
    "agg_time = time.time() - start_time\n",
    "\n",
    "print(f\"PySpark - Memory usage before filter: {mem_before_filter:.2f} MB\")\n",
    "print(f\"PySpark - Memory usage after filter: {mem_after_filter:.2f} MB\")\n",
    "print(f\"PySpark - Filter time: {filter_time:.4f} seconds\")\n",
    "print(f\"PySpark - Memory usage before aggregation: {mem_before_agg:.2f} MB\")\n",
    "print(f\"PySpark - Memory usage after aggregation: {mem_after_agg:.2f} MB\")\n",
    "print(f\"PySpark - Aggregation time: {agg_time:.4f} seconds\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343fcfe8-8a37-407e-a66a-9c2bd2123e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Pandas Internal Working\n",
    "Pandas is a single-node, in-memory data processing library built on top of NumPy. It works efficiently for smaller datasets \n",
    "that fit into your machine's memory.\n",
    "\n",
    "How Transformations Work in Pandas:\n",
    "\n",
    "Eager Evaluation: Pandas applies transformations immediately when you call a function. Every time you perform an operation (like filter, groupby, etc.), it processes the data and returns the result right away.\n",
    "In-Memory Processing: All operations are performed in memory. This makes Pandas very fast for small to medium datasets but memory-bound for larger datasets.\n",
    "Optimization: Limited internal optimization. You have control over optimizing your code (vectorization, avoiding loops, etc.).\n",
    "\n",
    "2. PySpark Internal Working\n",
    "PySpark is the Python API for Apache Spark, which is a distributed data processing engine. It works across multiple machines \n",
    "(nodes) and is designed for big data processing.\n",
    "\n",
    "How Transformations Work in PySpark:\n",
    "\n",
    "Lazy Evaluation: When you apply transformations (like filter, select, groupBy), PySpark doesn't execute them immediately. Instead, it builds a DAG (Directed Acyclic Graph) representing the sequence of transformations.\n",
    "Actions Trigger Execution: The actual computation only happens when an action (like collect(), show(), or write()) is called. At that point, Spark optimizes the DAG and executes the tasks across the cluster.\n",
    "Optimization with Catalyst and Tungsten:\n",
    "Catalyst Optimizer: Optimizes query plans before execution.\n",
    "Tungsten Engine: Provides efficient memory management and code generation for better performance.\n",
    "\n",
    "3. Performance Comparison:\n",
    "\n",
    "Pandas:\n",
    "\n",
    "Fast for small to medium datasets (fits into RAM).\n",
    "Time increases significantly as the dataset grows beyond memory capacity.\n",
    "Immediate execution after each step.\n",
    "\n",
    "PySpark:\n",
    "\n",
    "Overhead from starting the Spark context and distributing data.\n",
    "Slower for small datasets due to this overhead.\n",
    "Scales efficiently for large datasets because of parallel processing.\n",
    "Optimizes transformations before execution, reducing redundant computations.\n",
    "\n",
    "When to Use What?\n",
    "Pandas: Ideal for datasets that fit into memory (~1-2 GB), quick prototyping, and simple data analysis.\n",
    "PySpark: Best for big data scenarios where datasets are large (GBs to TBs), and distributed processing across clusters is required."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
