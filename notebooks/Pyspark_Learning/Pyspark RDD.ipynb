{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364021b4-3cb6-4f52-a097-e22dc1ec7355",
   "metadata": {},
   "outputs": [],
   "source": [
    "What is RDD (Resilient Distributed Dataset)?\n",
    "RDD, or Resilient Distributed Dataset, serves as a core component within PySpark, offering a fault-tolerant, \n",
    "distributed collection of objects. This foundational element boasts immutability, ensuring that once an RDD is created, \n",
    "it remains unchanged. Furthermore, RDDs are partitioned logically, facilitating parallel computation across various nodes \n",
    "within the cluster.\n",
    "\n",
    "RDDs are collections of objects similar to a list in Python; the difference is that RDD is computed on several processes scattered \n",
    "across multiple physical servers, also called nodes in a cluster, while a Python collection lives and processes in just one process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b9a443-8aff-49e4-abfc-2b954ec36ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PySpark RDD Benefits\n",
    "PySpark is widely adopted in the Machine learning and Data science community due to its advantages over traditional Python programming.\n",
    "\n",
    "In-Memory Processing\n",
    "PySpark loads the data from disk and processes it in memory, and keeps the data in memory; this is the main difference between PySpark and MapReduce (I/O intensive). In between the transformations, we can also cache/persists the RDD in memory to reuse the previous computations.\n",
    "\n",
    "Immutability\n",
    "PySpark RDDs are immutable in nature meaning, once RDDs are created you cannot modify them. When we apply transformations on RDD, PySpark creates a new RDD and maintains the RDD Lineage.\n",
    "\n",
    "Fault Tolerance\n",
    "PySpark operates on fault-tolerant data stores on HDFS, S3 e.t.c. Hence, if any RDD operation fails, it automatically reloads the data from other partitions. Also, when PySpark applications running on a cluster, PySpark task failures are automatically recovered for a certain number of times (as per the configuration) and finish the application seamlessly.\n",
    "\n",
    "Lazy Evolution\n",
    "PySpark does not evaluate the RDD transformations as they appear/encountered by Driver instead it keeps the all transformations as it encounters(DAG) and evaluates the all transformation when it sees the first RDD action.\n",
    "\n",
    "Partitioning\n",
    "When you create RDD from a data, It by default partitions the elements in a RDD. By default it partitions to the number of cores available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab02015c-c2a8-4037-b23d-af9cef2baf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD Creation\n",
    "You can create RDD by parallelizing the existing collection and reading data from a disk.\n",
    "\n",
    "1.parallelizing an existing collection and\n",
    "2.referencing a dataset in an external storage system \n",
    "\n",
    "Using sparkContext.parallelize()\n",
    "By using parallelize() function of SparkContext (sparkContext.parallelize() ) you can create an RDD. This function loads the existing collection from your driver program into parallelizing RDD. This method of creating an RDD is used when you already have data in memory that is either loaded from a file or from a database.\n",
    "and all data must be present in the driver program prior to creating RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05672420-07d1-4695-b8aa-f3870d246386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Spark session & context\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master(\"local\")\n",
    "         .appName(\"SparkbyExamples.com\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# Create RDD from parallelize    \n",
    "data = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "rdd = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f555ed-4503-4bea-9fa6-1c9306388dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Using sparkContext.textFile()\n",
    "Use the textFile() method to read a .txt file into RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcdb9bbb-7502-499b-b4aa-c22436fbf944",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create RDD from external Data source\n",
    "rdd2 = spark.sparkContext.textFile(\"/path/textFile.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1237aa23-4484-4198-b35a-8b9d41bec91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Using sparkContext.wholeTextFiles()\n",
    "wholeTextFiles() function returns a PairRDD with the key being the file path and the value being file content.\n",
    "Besides using text files, we can also create RDD from CSV file, JSON, and more formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c667f1d-052d-4f35-b347-889eb79cb07b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read entire file into a RDD as single record.\n",
    "rdd3 = spark.sparkContext.wholeTextFiles(\"/path/textFile.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edcead0-c2fc-4623-9242-58f4b8e4f70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Create empty RDD using sparkContext.emptyRDD\n",
    "Using emptyRDD() method on sparkContext we can create an RDD with no data. This method creates an empty RDD with no partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf305d75-04bf-4e89-aee3-1c7b40b63a09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an empty RDD with no partition    \n",
    "rdd = spark.sparkContext.emptyRDD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c70feb3b-49be-4da3-9379-d7e6efa14a7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating empty RDD with partition\n",
    "# Create empty RDD with partition\n",
    "rdd2 = spark.sparkContext.parallelize([],10) #This creates 10 partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e250477-6ec4-4d99-a22d-a403ffd14e8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial partition count:1\n"
     ]
    }
   ],
   "source": [
    "#getNumPartitions() â€“ This is an RDD function that returns a number of partitions your dataset split into.\n",
    "\n",
    "# Get partition count\n",
    "print(\"Initial partition count:\"+str(rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a64e783e-2160-4027-93b2-fe28d1e138a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[9] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set partitions manually\n",
    "spark.sparkContext.parallelize([1,2,3,4,56,7,8,9,12,3], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58ff31c9-1f1e-4bc9-926b-fb67bba7a758",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re-partition count:4\n"
     ]
    }
   ],
   "source": [
    "#6. Repartition and Coalesce\n",
    "#Sometimes, we may need to repartition the RDD, PySpark provides two ways to repartition; first using repartition() method, which shuffles data from all nodes also called full shuffle and second coalesce() method which shuffles data from minimum nodes, for examples if you have data in 4 partitions and doing coalesce(2) moves data from just 2 nodes.  \n",
    "#Both of these functions take the number of partitions to repartition RDD as shown below.  Note that repartition() method is a very expensive operation as it shuffles data from all nodes in a cluster. \n",
    "\n",
    "# Repartition the RDD\n",
    "reparRdd = rdd.repartition(4)\n",
    "print(\"re-partition count:\"+str(reparRdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37433dd-d447-4b2c-8c8c-fd90c9a8aa65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
