{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "314afa43-59b9-470d-92e3-ed15bbc8a13e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Repartition - increases or decreases partitions\n",
    "colaesce - only decreases - doesnt do full shuffle like Repartition - good for best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature\tRepartition\tCoalesce\n",
    "Description\tAdjusts the number of partitions, redistributing data across the specified number of partitions.\tReduces the number of partitions without shuffling data, merging existing partitions.\n",
    "Full Shuffle\tYes\tNo\n",
    "Expensiveness\tCan be expensive, especially for large datasets, as it involves a full shuffle of data.\tLess expensive than repartitioning, as it minimizes data movement by only combining partitions when possible.\n",
    "Data Movement\tDistributes data across partitions evenly, potentially leading to balanced partition sizes.\tMay result in imbalanced partition sizes, especially when reducing the number of partitions.\n",
    "Use Cases\tUseful when changing the number of partitions or evenly distributing data across partitions.\tUseful when decreasing the number of partitions without incurring the cost of a full shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "221fad43-8975-4469-8af6-b4bdaec6afce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[1]: 8"
     ]
    }
   ],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e369130b-88f3-43bb-b705-2fc50489d480",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: '134217728b'"
     ]
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.files.maxPartitionBytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ae98fcf-b981-48b5-9560-8d598e93876d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generating data in spark environment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: 8"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df=spark.createDataFrame(range(10),IntegerType())\n",
    "\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b16c8d93-58ad-4302-a007-66434d85005b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Verify the data in the partitions"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: [[Row(value=0)],\n",
      " [Row(value=1)],\n",
      " [Row(value=2)],\n",
      " [Row(value=3), Row(value=4)],\n",
      " [Row(value=5)],\n",
      " [Row(value=6)],\n",
      " [Row(value=7)],\n",
      " [Row(value=8), Row(value=9)]]"
     ]
    }
   ],
   "source": [
    "df.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c59d803a-b4ef-45cb-83ed-aaceb48a522f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: [FileInfo(path='dbfs:/FileStore/tables/stream_read/Online_sales1.csv', name='Online_sales1.csv', size=105, modificationTime=1729725236000),\n",
      " FileInfo(path='dbfs:/FileStore/tables/stream_read/Online_sales2.csv', name='Online_sales2.csv', size=105, modificationTime=1729725286000),\n",
      " FileInfo(path='dbfs:/FileStore/tables/stream_read/Online_sales3.csv', name='Online_sales3.csv', size=105, modificationTime=1729725301000),\n",
      " FileInfo(path='dbfs:/FileStore/tables/stream_read/Online_sales4.csv', name='Online_sales4.csv', size=105, modificationTime=1729725306000),\n",
      " FileInfo(path='dbfs:/FileStore/tables/stream_read/Online_sales5.csv', name='Online_sales5.csv', size=105, modificationTime=1729725310000)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls(\"/FileStore/tables/stream_read/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b14c8043-4c8b-497b-83ba-d37453c6e8c6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read external file in spark"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: 5"
     ]
    }
   ],
   "source": [
    "df=spark.read.format(\"csv\").option(\"header\",True).option(\"inferSchema\",True).option(\"sep\",\";\").load(\"/FileStore/tables/stream_read/\")\n",
    "\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa7c4a2f-951f-472b-b617-deb4900ab4f0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Change maxpartitionbytes parameter which changes no. of partitions"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[17]: '100000'"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\",100000)\n",
    "spark.conf.get(\"spark.sql.files.maxPartitionBytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86cdd790-4ef8-4f2d-b471-751e351c5884",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[18]: 5"
     ]
    }
   ],
   "source": [
    "df=spark.read.format(\"csv\").option(\"header\",True).option(\"inferSchema\",True).option(\"sep\",\";\").load(\"/FileStore/tables/stream_read/\")\n",
    "\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "210509e2-7ac4-4473-9722-2edba54a3e56",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Repartition increase"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[19]: [[Row(value=0)],\n",
      " [Row(value=1)],\n",
      " [Row(value=2)],\n",
      " [Row(value=3), Row(value=4)],\n",
      " [Row(value=5)],\n",
      " [Row(value=6)],\n",
      " [Row(value=7)],\n",
      " [Row(value=8), Row(value=9)]]"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df=spark.createDataFrame(range(10),IntegerType())\n",
    "\n",
    "df.rdd.getNumPartitions()\n",
    "df.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "199a1f81-a312-4e75-8896-591f11b01017",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[20]: 20"
     ]
    }
   ],
   "source": [
    "df1 = df.repartition(20)\n",
    "df1.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "870b2228-5b3d-4fd9-ad25-e0680ec2cf04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[21]: [[],\n",
      " [Row(value=8)],\n",
      " [Row(value=9)],\n",
      " [],\n",
      " [Row(value=1)],\n",
      " [],\n",
      " [Row(value=6)],\n",
      " [],\n",
      " [Row(value=3)],\n",
      " [Row(value=0), Row(value=2), Row(value=4)],\n",
      " [],\n",
      " [Row(value=7)],\n",
      " [],\n",
      " [],\n",
      " [Row(value=5)],\n",
      " [],\n",
      " [],\n",
      " [],\n",
      " [],\n",
      " []]"
     ]
    }
   ],
   "source": [
    "df1.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ff35a63-fde7-4e7a-92dc-f8afef1d2087",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[24]: 2"
     ]
    }
   ],
   "source": [
    "df1=df.repartition(2)\n",
    "df1.rdd.getNumPartitions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "985e10ac-ef1f-4ab7-869b-e7183555a8b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[25]: [[Row(value=2),\n",
      "  Row(value=3),\n",
      "  Row(value=5),\n",
      "  Row(value=6),\n",
      "  Row(value=7),\n",
      "  Row(value=9)],\n",
      " [Row(value=0), Row(value=1), Row(value=4), Row(value=8)]]"
     ]
    }
   ],
   "source": [
    "df1.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3d6c622-914c-482a-ae47-2a80d22916e5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Coalesce"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[29]: [[Row(value=0), Row(value=1), Row(value=2), Row(value=3), Row(value=4)],\n",
      " [Row(value=5), Row(value=6), Row(value=7), Row(value=8), Row(value=9)]]"
     ]
    }
   ],
   "source": [
    "df2 = df.coalesce(2)\n",
    "df2.rdd.getNumPartitions()\n",
    "df2.rdd.glom().collect()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Repartition Coalesce",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
