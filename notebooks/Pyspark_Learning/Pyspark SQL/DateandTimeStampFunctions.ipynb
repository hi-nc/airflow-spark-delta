{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88de3c01-448c-49c5-88b9-157ec59bff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "PySpark Date and Timestamp Functions are supported on DataFrame and SQL queries and they work similarly to traditional SQL, \n",
    "Date and Time are very important if you are using PySpark for ETL. Most of all these functions accept input as, Date type, \n",
    "Timestamp type, or String. If a String used, it should be in a default format that can be cast to date.\n",
    "\n",
    "DateType default format is yyyy-MM-dd \n",
    "TimestampType default format is yyyy-MM-dd HH:mm:ss.SSSS\n",
    "Returns null if the input is a string that can not be cast to Date or Timestamp.\n",
    "\n",
    "These are compile-time safe, handles null, and perform better when compared to PySpark UDF.\n",
    "If your PySpark application is critical on performance ,try to avoid using custom UDF at all costs as these are not \n",
    "guarantee performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "631bd1c7-8005-4da7-87a1-3e5e8b54d3ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|     input|\n",
      "+---+----------+\n",
      "|  1|2020-02-01|\n",
      "|  2|2019-03-01|\n",
      "|  3|2021-03-01|\n",
      "+---+----------+\n",
      "\n",
      "+------------+\n",
      "|current_date|\n",
      "+------------+\n",
      "|  2025-02-08|\n",
      "+------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+-----------+\n",
      "|     input|date_format|\n",
      "+----------+-----------+\n",
      "|2020-02-01| 02-01-2020|\n",
      "|2019-03-01| 03-01-2019|\n",
      "|2021-03-01| 03-01-2021|\n",
      "+----------+-----------+\n",
      "\n",
      "+----------+----------+\n",
      "|     input|   to_date|\n",
      "+----------+----------+\n",
      "|2020-02-01|2020-02-01|\n",
      "|2019-03-01|2019-03-01|\n",
      "|2021-03-01|2021-03-01|\n",
      "+----------+----------+\n",
      "\n",
      "+----------+--------+\n",
      "|     input|datediff|\n",
      "+----------+--------+\n",
      "|2020-02-01|    1834|\n",
      "|2019-03-01|    2171|\n",
      "|2021-03-01|    1440|\n",
      "+----------+--------+\n",
      "\n",
      "+----------+--------------+\n",
      "|     input|months_between|\n",
      "+----------+--------------+\n",
      "|2020-02-01|   60.22580645|\n",
      "|2019-03-01|   71.22580645|\n",
      "|2021-03-01|   47.22580645|\n",
      "+----------+--------------+\n",
      "\n",
      "+----------+-----------+----------+-----------+\n",
      "|     input|Month_Trunc|Month_Year|Month_Trunc|\n",
      "+----------+-----------+----------+-----------+\n",
      "|2020-02-01| 2020-02-01|2020-01-01| 2020-02-01|\n",
      "|2019-03-01| 2019-03-01|2019-01-01| 2019-03-01|\n",
      "|2021-03-01| 2021-03-01|2021-01-01| 2021-03-01|\n",
      "+----------+-----------+----------+-----------+\n",
      "\n",
      "+----------+----------+----------+----------+----------+\n",
      "|     input|add_months|sub_months|  date_add|  date_sub|\n",
      "+----------+----------+----------+----------+----------+\n",
      "|2020-02-01|2020-05-01|2019-11-01|2020-02-05|2020-01-28|\n",
      "|2019-03-01|2019-06-01|2018-12-01|2019-03-05|2019-02-25|\n",
      "|2021-03-01|2021-06-01|2020-12-01|2021-03-05|2021-02-25|\n",
      "+----------+----------+----------+----------+----------+\n",
      "\n",
      "+----------+----+-----+----------+----------+\n",
      "|     input|year|month|  next_day|weekofyear|\n",
      "+----------+----+-----+----------+----------+\n",
      "|2020-02-01|2020|    2|2020-02-02|         5|\n",
      "|2019-03-01|2019|    3|2019-03-03|         9|\n",
      "|2021-03-01|2021|    3|2021-03-07|         9|\n",
      "+----------+----+-----+----------+----------+\n",
      "\n",
      "+----------+---------+----------+---------+\n",
      "|     input|dayofweek|dayofmonth|dayofyear|\n",
      "+----------+---------+----------+---------+\n",
      "|2020-02-01|        7|         1|       32|\n",
      "|2019-03-01|        6|         1|       60|\n",
      "|2021-03-01|        2|         1|       60|\n",
      "+----------+---------+----------+---------+\n",
      "\n",
      "+---+-----------------------+\n",
      "|id |input                  |\n",
      "+---+-----------------------+\n",
      "|1  |02-01-2020 11 01 19 06 |\n",
      "|2  |03-01-2019 12 01 19 406|\n",
      "|3  |03-01-2021 12 01 19 406|\n",
      "+---+-----------------------+\n",
      "\n",
      "+--------------------------+\n",
      "|current_timestamp         |\n",
      "+--------------------------+\n",
      "|2025-02-08 20:49:06.320615|\n",
      "+--------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-----------------------+-----------------------+\n",
      "|input                  |to_timestamp           |\n",
      "+-----------------------+-----------------------+\n",
      "|02-01-2020 11 01 19 06 |2020-02-01 11:01:19.06 |\n",
      "|03-01-2019 12 01 19 406|2019-03-01 12:01:19.406|\n",
      "|03-01-2021 12 01 19 406|2021-03-01 12:01:19.406|\n",
      "+-----------------------+-----------------------+\n",
      "\n",
      "+-----------------------+----+------+------+\n",
      "|input                  |hour|minute|second|\n",
      "+-----------------------+----+------+------+\n",
      "|2020-02-01 11:01:19.06 |11  |1     |19    |\n",
      "|2019-03-01 12:01:19.406|12  |1     |19    |\n",
      "|2021-03-01 12:01:19.406|12  |1     |19    |\n",
      "+-----------------------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "data=[[\"1\",\"2020-02-01\"],[\"2\",\"2019-03-01\"],[\"3\",\"2021-03-01\"]]\n",
    "df=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "df.show()\n",
    "\n",
    "#current_date()\n",
    "df.select(current_date().alias(\"current_date\")).show(1)\n",
    "\n",
    "#date_format()\n",
    "df.select(col(\"input\"), date_format(col(\"input\"), \"MM-dd-yyyy\").alias(\"date_format\")).show()\n",
    "\n",
    "#to_date()\n",
    "df.select(col(\"input\"), to_date(col(\"input\"), \"yyy-MM-dd\").alias(\"to_date\") ).show()\n",
    "\n",
    "#datediff()\n",
    "df.select(col(\"input\"), datediff(current_date(),col(\"input\")).alias(\"datediff\")).show()\n",
    "\n",
    "#months_between()\n",
    "df.select(col(\"input\"), months_between(current_date(),col(\"input\")).alias(\"months_between\")).show()\n",
    "\n",
    "#trunc()- # Truncate to the beginning of the month, year\n",
    "df.select(col(\"input\"), \n",
    "    trunc(col(\"input\"),\"Month\").alias(\"Month_Trunc\"), \n",
    "    trunc(col(\"input\"),\"Year\").alias(\"Month_Year\"), \n",
    "    trunc(col(\"input\"),\"Month\").alias(\"Month_Trunc\")).show()\n",
    "\n",
    "#add_months() , date_add(), date_sub()\n",
    "df.select(col(\"input\"), \n",
    "    add_months(col(\"input\"),3).alias(\"add_months\"), \n",
    "    add_months(col(\"input\"),-3).alias(\"sub_months\"), \n",
    "    date_add(col(\"input\"),4).alias(\"date_add\"), \n",
    "    date_sub(col(\"input\"),4).alias(\"date_sub\")).show()\n",
    "\n",
    "df.select(col(\"input\"), \n",
    "     year(col(\"input\")).alias(\"year\"), \n",
    "     month(col(\"input\")).alias(\"month\"), \n",
    "     next_day(col(\"input\"),\"Sunday\").alias(\"next_day\"), \n",
    "     weekofyear(col(\"input\")).alias(\"weekofyear\") ).show()\n",
    "\n",
    "df.select(col(\"input\"),  \n",
    "     dayofweek(col(\"input\")).alias(\"dayofweek\"), \n",
    "     dayofmonth(col(\"input\")).alias(\"dayofmonth\"), \n",
    "     dayofyear(col(\"input\")).alias(\"dayofyear\")).show()\n",
    "\n",
    "data=[[\"1\",\"02-01-2020 11 01 19 06\"],[\"2\",\"03-01-2019 12 01 19 406\"],[\"3\",\"03-01-2021 12 01 19 406\"]]\n",
    "df2=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "df2.show(truncate=False)\n",
    "\n",
    "#current_timestamp()-returns the current timestamp in spark default format yyyy-MM-dd HH:mm:ss\n",
    "df2.select(current_timestamp().alias(\"current_timestamp\")).show(1,truncate=False)\n",
    "\n",
    "#to_timestamp()-Converts string timestamp to Timestamp type format.\n",
    "df2.select(col(\"input\"), \n",
    "    to_timestamp(col(\"input\"), \"MM-dd-yyyy HH mm ss SSS\").alias(\"to_timestamp\")  ).show(truncate=False)\n",
    "\n",
    "#hour(), Minute() and second()\n",
    "data=[[\"1\",\"2020-02-01 11:01:19.06\"],[\"2\",\"2019-03-01 12:01:19.406\"],[\"3\",\"2021-03-01 12:01:19.406\"]]\n",
    "df3=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "\n",
    "df3.select(col(\"input\"), \n",
    "    hour(col(\"input\")).alias(\"hour\"), \n",
    "    minute(col(\"input\")).alias(\"minute\"),\n",
    "    second(col(\"input\")).alias(\"second\") \n",
    "  ).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a5549d-1d30-41d0-90a5-a3fb7d461361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
