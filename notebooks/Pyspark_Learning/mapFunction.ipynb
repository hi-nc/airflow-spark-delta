{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fae4b8-d98d-42ed-b8d1-eff904f6ebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "The map()in PySpark is a transformation function that is used to apply a function/lambda to each element of an RDD \n",
    "(Resilient Distributed Dataset) and return a new RDD consisting of the result.\n",
    "When you have complex operations to apply on an RDD, the map() transformation is defacto function. \n",
    "You can use this for simple to complex operations like deriving a new element from exising data, or transforming the data, etc;\n",
    "DataFrame doesn’t have map() transformation to use with DataFrame; hence, you need to convert DataFrame to RDD first.\n",
    "\n",
    "map() transformation works as follows\n",
    "Function Application: You define a function that you want to apply to each element of the RDD.\n",
    "Function Application to RDD: You call the map() transformation on the RDD and pass the function as an argument to it.\n",
    "Transformation Execution: Spark applies the provided function to each element of the RDD in a distributed manner across the cluster.\n",
    "New RDD Creation: The map() transformation returns a new RDD containing the results of applying the function to each element of the original RDD.\n",
    "\n",
    "The map() transformation in PySpark processes each element independently, and by default, it does not handle the null or \n",
    "missing values. We need to handle these cases within the mapping function explicitly\n",
    "The map() transformation applies a function on each element of the RDD independently, resulting in a new RDD with the same \n",
    "number of elements. Meanwhile, flatMap() can produce a variable number of output elements for each input element.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68444258-3b0c-4cc7-bb3a-1c4ca3ed6b68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Project', 1)\n",
      "('Gutenberg’s', 1)\n",
      "('Alice’s', 1)\n",
      "('Adventures', 1)\n",
      "('in', 1)\n",
      "('Wonderland', 1)\n",
      "('Project', 1)\n",
      "('Gutenberg’s', 1)\n",
      "('Adventures', 1)\n",
      "('in', 1)\n",
      "('Wonderland', 1)\n",
      "('Project', 1)\n",
      "('Gutenberg’s', 1)\n",
      "+---------+--------+------+------+\n",
      "|firstname|lastname|gender|salary|\n",
      "+---------+--------+------+------+\n",
      "|    James|   Smith|     M|    30|\n",
      "|     Anna|    Rose|     F|    41|\n",
      "|   Robert|Williams|     M|    62|\n",
      "+---------+--------+------+------+\n",
      "\n",
      "+---------------+------+----------+\n",
      "|           name|gender|new_salary|\n",
      "+---------------+------+----------+\n",
      "|    James,Smith|     M|        60|\n",
      "|      Anna,Rose|     F|        82|\n",
      "|Robert,Williams|     M|       124|\n",
      "+---------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "data = [\"Project\",\n",
    "\"Gutenberg’s\",\n",
    "\"Alice’s\",\n",
    "\"Adventures\",\n",
    "\"in\",\n",
    "\"Wonderland\",\n",
    "\"Project\",\n",
    "\"Gutenberg’s\",\n",
    "\"Adventures\",\n",
    "\"in\",\n",
    "\"Wonderland\",\n",
    "\"Project\",\n",
    "\"Gutenberg’s\"]\n",
    "\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "\n",
    "rdd2=rdd.map(lambda x: (x,1))\n",
    "for element in rdd2.collect():\n",
    "    print(element)\n",
    "    \n",
    "data = [('James','Smith','M',30),\n",
    "  ('Anna','Rose','F',41),\n",
    "  ('Robert','Williams','M',62), \n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "df.show()\n",
    "\n",
    "rdd2=df.rdd.map(lambda x: \n",
    "    (x[0]+\",\"+x[1],x[2],x[3]*2)\n",
    "    )  \n",
    "df2=rdd2.toDF([\"name\",\"gender\",\"new_salary\"]   )\n",
    "df2.show()\n",
    "\n",
    "#Referring Column Names\n",
    "rdd2=df.rdd.map(lambda x: \n",
    "    (x[\"firstname\"]+\",\"+x[\"lastname\"],x[\"gender\"],x[\"salary\"]*2)\n",
    "    ) \n",
    "\n",
    "#Referring Column Names\n",
    "rdd2=df.rdd.map(lambda x: \n",
    "    (x.firstname+\",\"+x.lastname,x.gender,x.salary*2)\n",
    "    ) \n",
    "\n",
    "def func1(x):\n",
    "    firstName=x.firstname\n",
    "    lastName=x.lastname\n",
    "    name=firstName+\",\"+lastName\n",
    "    gender=x.gender.lower()\n",
    "    salary=x.salary*2\n",
    "    return (name,gender,salary)\n",
    "\n",
    "rdd2=df.rdd.map(lambda x: func1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bfbf59-2d04-4e77-bfc7-fa18d64a9d44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
