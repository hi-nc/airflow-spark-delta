{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aef4bfb-8927-4b41-865f-ef739bebe33c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"SparkByExamples.com\") \\\n",
    "      .getOrCreate() \n",
    "\n",
    "# Create RDD from external Data source\n",
    "rdd2 = spark.sparkContext.textFile(\"/path/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c10bb1-3c4e-40cb-9be8-d7c2055b087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD Transformations\n",
    "Spark RDD Transformations are lazy operations meaning they don’t execute until you call an action on RDD. Since RDDs are immutable, When you run a transformation(for example map()), instead of updating a current RDD, it returns a new RDD.\n",
    "\n",
    "Some transformations on RDDs are flatMap(), map(), reduceByKey(), filter(), sortByKey() and all these return a new RDD instead of updating the current.\n",
    "\n",
    "RDD Actions\n",
    "RDD Action operation returns the values from an RDD to a driver node. In other words, any RDD function that returns non RDD[T] is considered as an action. RDD operations trigger the computation and return RDD in a List to the driver program.\n",
    "\n",
    "Some actions on RDDs are count(),  collect(),  first(),  max(),  reduce()  and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e66a6e-7b60-4701-86d9-0c1a868c1167",
   "metadata": {},
   "outputs": [],
   "source": [
    "How to create SparkSession?\n",
    "SparkSession is created using SparkSession.builder().master(\"master-details\").appName(\"app-name\").getOrCreate(); Here, getOrCreate() method returns SparkSession if already exists. If not, it creates a new SparkSession.\n",
    "\n",
    "How many SparkSessions can I create?\n",
    "You can create as many SparkSession as you want in a Spark application using either SparkSession.builder() or SparkSession.newSession(). Many Spark session objects are required when you want to keep Spark tables (relational entities) logically separated.\n",
    "\n",
    "How to stop SparkSession?\n",
    "To stop SparkSession in Apache Spark, you can use the stop() method of the SparkSession object. If you have spark as a SparkSession object then call spark.stop() to stop the session. Calling a stop() is important to do when you’re finished with your Spark application. This ensures that resources are properly released and the Spark application terminates gracefully.\n",
    "\n",
    "How SparkSession is different from SparkContext?\n",
    "SparkSession and SparkContext are two core components of Apache Spark. Though they sound similar, they serve different purposes and are used in different contexts within a Spark application.\n",
    "SparkContext provides the connection to a Spark cluster and is responsible for coordinating and distributing the operations on that cluster. SparkContext is used for low-level RDD (Resilient Distributed Dataset) programming.\n",
    "SparkSession was introduced in Spark 2.0 to provide a more convenient and unified API for working with structured data. It’s designed to work with DataFrames and Datasets, which provide more structured and optimized operations than RDDs.\n",
    "\n",
    "Do we need to stop SparkSession?\n",
    "It is recommended to end the Spark session after finishing the Spark job in order for the JVMs to close and free the resources. \n",
    "\n",
    "How do I know if my Spark session is active?\n",
    "To check if your SparkSession is active, you can use the SparkSession object’s sparkContext attribute and check its isActive property. If you have spark as a SparkSession object then call spark.sparkContext.isActive. This returns true if it is active otherwise false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3903703-eb82-4bbb-97a0-0a383c1eb5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "What are the differences between Pandas and PySpark DataFrame? Pandas and PySpark are both powerful tools for data manipulation and \n",
    "analysis in Python. Pandas is a widely-used library for working with smaller datasets in memory on a single machine, offering a \n",
    "rich set of functions for data manipulation and analysis. In contrast, PySpark, built on top of Apache Spark, is designed for \n",
    "distributed computing, allowing for the processing of massive datasets across multiple machines in a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9f040a-baee-4ba8-8b09-bc11c68379bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pandas is one of the most used open-source Python libraries to work with Structured tabular data for analysis. Pandas library is heavily used for Data Analytics, Machine learning, data science projects, and many more.\n",
    "Pandas can load the data by reading CSV, JSON, SQL, many other formats and creates a DataFrame which is a structured object containing rows and columns (similar to SQL table).\n",
    "It doesn’t support distributed processing; hence, you would always need to increase the resources when you need additional horsepower to support your growing data.\n",
    "Pandas DataFrames are mutable and are not lazy, statistical functions are applied on each column by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beab34cd-4098-4359-a787-5077c873041d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  First Name Middle Name Last Name  Age Gender  Salary\n",
      "0      James                 Smith   30      M   60000\n",
      "1    Michael        Rose             50      M   70000\n",
      "2     Robert              Williams   42         400000\n",
      "3      Maria        Anne     Jones   38      F  500000\n",
      "4        Jen        Mary     Brown   45   None       0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd    \n",
    "data = [[\"James\",\"\",\"Smith\",30,\"M\",60000], \n",
    "        [\"Michael\",\"Rose\",\"\",50,\"M\",70000], \n",
    "        [\"Robert\",\"\",\"Williams\",42,\"\",400000], \n",
    "        [\"Maria\",\"Anne\",\"Jones\",38,\"F\",500000], \n",
    "        [\"Jen\",\"Mary\",\"Brown\",45,None,0]] \n",
    "columns=['First Name','Middle Name','Last Name','Age','Gender','Salary']\n",
    "\n",
    "# Create the pandas DataFrame \n",
    "pandasDF=pd.DataFrame(data=data, columns=columns) \n",
    "  \n",
    "# print dataframe. \n",
    "print(pandasDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803a2eb7-b11b-4b24-aa8b-f18f117d3177",
   "metadata": {},
   "outputs": [],
   "source": [
    "Note that Pandas add an index sequence number to every data frame.Below are some Pandas transformations.\n",
    "df.count() – Returns the count of each column (the count includes only non-null values).\n",
    "df.corr() – Returns the correlation between columns in a data frame.\n",
    "df.head(n) – Returns first n rows from the top.\n",
    "df.max() – Returns the maximum of each column.\n",
    "df.mean() – Returns the mean of each column.\n",
    "df.median() – Returns the median of each column.\n",
    "df.min() – Returns the minimum value in each column.\n",
    "df.std() – Returns the standard deviation of each column\n",
    "df.tail(n) – Returns last n rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7debe075-14ad-4d8b-a3e3-5443fdc3b072",
   "metadata": {},
   "outputs": [],
   "source": [
    "PySpark is a Python API for Apache Spark, a distributed computing framework designed for processing large-scale datasets across clusters \n",
    "of machines. PySpark enables parallelized data processing and analysis by distributing computations across multiple nodes in a cluster, \n",
    "providing scalability and high performance for big data analytics tasks. It offers a DataFrame API that resembles Pandas, allowing users \n",
    "to perform similar data manipulation operations but on distributed datasets.\n",
    "In comparison, PySpark is designed for handling large-scale datasets that exceed the memory capacity of a single machine, making it \n",
    "suitable for big data analytics tasks that require distributed computing capabilities. While both PySpark and Pandas offer similar \n",
    "DataFrame APIs and data manipulation functionalities, PySpark’s distributed architecture provides scalability and parallelism for \n",
    "processing massive datasets across distributed clusters. Ultimately, the choice between PySpark and Pandas depends on the scale of \n",
    "the datasets and the computational resources available for data analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f10f4ca-a528-4fd6-80d5-2227fc481656",
   "metadata": {},
   "outputs": [],
   "source": [
    "PySpark DataFrame is immutable (cannot be changed once created), fault-tolerant and Transformations are \n",
    "Lazy evaluation (they are not executed until actions are called). PySpark DataFrames are distributed in the cluster \n",
    "(meaning the data in PySpark DataFrames are stored in different machines in a cluster) and any operations in PySpark execute in parallel\n",
    "on all machines.\n",
    "\n",
    "Below are some pyspark transformations.\n",
    "select() – Choose specific columns from a DataFrame.\n",
    "filter() – Filter rows based on a condition.\n",
    "groupBy() – Group rows based on one or more columns.\n",
    "agg() – Perform aggregate functions (e.g., sum, average) on grouped data.\n",
    "orderBy() – Sort rows based on one or more columns.\n",
    "dropDuplicates() – Remove duplicate rows from the DataFrame.\n",
    "withColumn() – Add a new column or replace an existing column with modified data.\n",
    "drop() – Remove one or more columns from the DataFrame.\n",
    "join() – Merge two DataFrames based on a common column or index.\n",
    "pivot() – Pivot the DataFrame to reorganize data based on column values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7031ce8d-7a55-4bde-aec9-8d307f5dbb7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- middle_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+----------+-----------+---------+---+------+------+\n",
      "|first_name|middle_name|last_name|Age|gender|salary|\n",
      "+----------+-----------+---------+---+------+------+\n",
      "|James     |           |Smith    |30 |M     |60000 |\n",
      "|Michael   |Rose       |         |50 |M     |70000 |\n",
      "|Robert    |           |Williams |42 |      |400000|\n",
      "|Maria     |Anne       |Jones    |38 |F     |500000|\n",
      "|Jen       |Mary       |Brown    |45 |F     |0     |\n",
      "+----------+-----------+---------+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "               .appName('SparkByExamples.com') \\\n",
    "               .getOrCreate()\n",
    "\n",
    "data = [(\"James\",\"\",\"Smith\",30,\"M\",60000),\n",
    "        (\"Michael\",\"Rose\",\"\",50,\"M\",70000),\n",
    "        (\"Robert\",\"\",\"Williams\",42,\"\",400000),\n",
    "        (\"Maria\",\"Anne\",\"Jones\",38,\"F\",500000),\n",
    "        (\"Jen\",\"Mary\",\"Brown\",45,\"F\",0)]\n",
    "\n",
    "columns = [\"first_name\",\"middle_name\",\"last_name\",\"Age\",\"gender\",\"salary\"]\n",
    "pysparkDF = spark.createDataFrame(data = data, schema = columns)\n",
    "pysparkDF.printSchema()\n",
    "pysparkDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02931d74-9b13-47cb-be01-c0454169d993",
   "metadata": {},
   "outputs": [],
   "source": [
    "PySpark supports SQL queries to run transformations. All you need to do is create a Table/View from the PySpark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac6e14d9-7f04-4aa9-b788-5daaad5c7389",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------+---+------+------+\n",
      "|first_name|middle_name|last_name|Age|gender|salary|\n",
      "+----------+-----------+---------+---+------+------+\n",
      "|    Robert|           | Williams| 42|      |400000|\n",
      "|     Maria|       Anne|    Jones| 38|     F|500000|\n",
      "+----------+-----------+---------+---+------+------+\n",
      "\n",
      "+---------+------------+\n",
      "|mean(age)|mean(salary)|\n",
      "+---------+------------+\n",
      "|     41.0|    206000.0|\n",
      "+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySpark SQL\n",
    "pysparkDF.createOrReplaceTempView(\"Employee\")\n",
    "spark.sql(\"select * from Employee where salary > 100000\").show()\n",
    "spark.sql(\"select mean(age),mean(salary) from Employee\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6076a8f-f2f1-4fb7-a2b5-ece7eb8a85d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Create PySpark DataFrame from Pandas\n",
    "Due to parallel execution on all cores on multiple machines, PySpark runs operations faster than Pandas, hence we often required to covert Pandas DataFrame to PySpark (Spark with Python) for better performance. This is one of the major differences between Pandas vs PySpark DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28a3d6f1-b305-46b5-bec6-8ce5cbc80a7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'iteritems'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create PySpark DataFrame from Pandas\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m pysparkDF2 \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpandasDF\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      3\u001b[0m pysparkDF2\u001b[38;5;241m.\u001b[39mprintSchema()\n\u001b[1;32m      4\u001b[0m pysparkDF2\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:891\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    888\u001b[0m     has_pandas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m--> 891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(\n\u001b[1;32m    895\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    896\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/conversion.py:436\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    434\u001b[0m             warn(msg)\n\u001b[1;32m    435\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m--> 436\u001b[0m converted_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_from_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimezone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(converted_data, schema, samplingRatio, verifySchema)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/conversion.py:474\u001b[0m, in \u001b[0;36mSparkConversionMixin._convert_from_pandas\u001b[0;34m(self, pdf, schema, timezone)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    473\u001b[0m     should_localize \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m is_timestamp_ntz_preferred()\n\u001b[0;32m--> 474\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m column, series \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miteritems\u001b[49m():\n\u001b[1;32m    475\u001b[0m         s \u001b[38;5;241m=\u001b[39m series\n\u001b[1;32m    476\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m should_localize \u001b[38;5;129;01mand\u001b[39;00m is_datetime64tz_dtype(s\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m s\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mtz \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/generic.py:5989\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5983\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   5984\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   5985\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   5986\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5987\u001b[0m ):\n\u001b[1;32m   5988\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 5989\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'iteritems'"
     ]
    }
   ],
   "source": [
    "# Create PySpark DataFrame from Pandas\n",
    "pysparkDF2 = spark.createDataFrame(pandasDF) \n",
    "pysparkDF2.printSchema()\n",
    "pysparkDF2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8067807a-93e4-45ce-b243-153987d5f6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Create Pandas from PySpark DataFrame\n",
    "Once the transformations are done on Spark, you can easily convert it back to Pandas using toPandas() method.\n",
    "\n",
    "Note: toPandas() method is an action that collects the data into Spark Driver memory so you have to be very careful while\n",
    "dealing with large datasets. You will get OutOfMemoryException if the collected data doesn’t fit in Spark Driver memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2dda3ff-2e8e-405c-9327-7c1b79609d62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  first_name middle_name last_name  Age gender  salary\n",
      "0      James                 Smith   30      M   60000\n",
      "1    Michael        Rose             50      M   70000\n",
      "2     Robert              Williams   42         400000\n",
      "3      Maria        Anne     Jones   38      F  500000\n",
      "4        Jen        Mary     Brown   45      F       0\n"
     ]
    }
   ],
   "source": [
    "# Convert PySpark to Pandas\n",
    "pandasDF = pysparkDF.toPandas()\n",
    "print(pandasDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637668fc-0751-453a-a81f-9b76260d67db",
   "metadata": {},
   "outputs": [],
   "source": [
    "How to Decide Between Pandas vs PySpark\n",
    "Deciding between Pandas and PySpark depends on several factors, including the scale of the data, available computational resources, and specific requirements of the data analysis tasks. Here are some considerations to help you decide:\n",
    "\n",
    "Data Scale:\n",
    "Use Pandas for small to medium-sized datasets that fit into memory and require rapid in-memory data manipulation and analysis.\n",
    "Choose PySpark for large-scale datasets that exceed the memory capacity of a single machine and require distributed computing capabilities for parallelized data processing.\n",
    "Computational Resources:\n",
    "If you have limited computational resources or a single machine environment, Pandas may be more suitable due to its in-memory processing capabilities.\n",
    "For distributed computing environments with access to clusters of machines, PySpark offers scalability and parallelism for processing massive datasets across distributed clusters.\n",
    "Performance:\n",
    "Pandas performs well for small to medium-sized datasets but may struggle with large-scale datasets due to memory constraints.\n",
    "PySpark excels in processing large-scale datasets across distributed clusters, offering scalability and parallelism for improved performance.\n",
    "Ecosystem and Integration:\n",
    "Pandas has a mature ecosystem with extensive support for data manipulation, visualization, and analysis tools, making it suitable for a wide range of data analysis tasks.\n",
    "PySpark integrates with the broader Apache Spark ecosystem, offering support for various data sources, machine learning libraries, and streaming processing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd6ecfc-c847-40a7-9bf9-9552bb0b8d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "Python SparkContext:\n",
    "Once you have a SparkContext object, you can create a PySpark RDD in several ways, below I have used the range() function.\n",
    "When you try to create multiple SparkContext you will get the below error.\n",
    "\n",
    "ValueError: Cannot run multiple SparkContexts at once;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60ef6190-75a1-4ae7-82a9-e91f5f0afc69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Example App\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(sc\u001b[38;5;241m.\u001b[39mappName)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Create RDD\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m rdd \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39mrange(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(rdd\u001b[38;5;241m.\u001b[39mcollect())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# Create Spark Context\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local\").setAppName(\"Spark Example App\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "print(sc.appName)\n",
    "\n",
    "# Create RDD\n",
    "rdd = spark.sparkContext.range(1, 5)\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a8d13f-c7c2-4659-8049-1383337c9707",
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkContext Commonly Used Variables\n",
    "\n",
    "applicationId – Returns a unique ID of a PySpark application.\n",
    "version – Version of PySpark cluster where your job is running.\n",
    "uiWebUrl – Provides the Spark Web UI url that started by SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f6e0bb-eb87-47f5-ac3b-948f752112d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkContext Commonly Used Methods\n",
    "accumulator(value[, accum_param]) – It creates an pyspark accumulator variable with initial specified value. Only a driver can access accumulator variables.\n",
    "\n",
    "broadcast(value) – read-only PySpark broadcast variable. This will be broadcast to the entire cluster. You can broadcast a variable to a PySpark cluster only once.\n",
    "\n",
    "emptyRDD() – Creates an empty RDD\n",
    "\n",
    "getOrCreate() – Creates or returns a SparkContext\n",
    "\n",
    "hadoopFile() – Returns an RDD of a Hadoop file\n",
    "\n",
    "newAPIHadoopFile() – Creates an RDD for a Hadoop file with a new API InputFormat.\n",
    "\n",
    "sequenceFile() – Get an RDD for a Hadoop SequenceFile with given key and value types.\n",
    "\n",
    "setLogLevel() – Change log level to debug, info, warn, fatal, and error\n",
    "\n",
    "textFile() – Reads a text file from HDFS, local or any Hadoop supported file systems and returns an RDD\n",
    "\n",
    "union() – Union two RDDs\n",
    "\n",
    "wholeTextFiles() – Reads a text file in the folder from HDFS, local or any Hadoop supported file systems and returns an RDD of Tuple2. The first element of the tuple consists \n",
    "file name and the second element consists context of the text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758e0e96-4bbd-4982-862d-da06113f3f50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188cfe05-32ff-4e14-bba2-a316de35d635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567dbeaf-d2d2-4d47-945b-ba46653eed61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1a9ded-ebee-4b9e-a5ca-a6ddb85f7137",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
