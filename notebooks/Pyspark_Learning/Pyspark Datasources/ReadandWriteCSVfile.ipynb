{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20edb0cb-c154-4b2a-b21b-07a641349202",
   "metadata": {},
   "outputs": [],
   "source": [
    "PySpark can automatically infer the schema of CSV files, eliminating the need for manual schema definition in many cases.\n",
    "Users have the flexibility to define custom schemas for CSV files, specifying data types and column names as needed.\n",
    "PySpark offers options for handling headers in CSV files, allowing users to skip headers or treat them as data rows.\n",
    "Provides robust error handling mechanisms for dealing with malformed or corrupted CSV files, ensuring data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0b905aa-8bdd-4081-a677-4fdfb9dd7bf7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "# Read CSV File\n",
    "df = spark.read.csv(\"/home/jovyan/work/data/zipcodes.csv\")\n",
    "df.printSchema()\n",
    "\n",
    "# Using format().load()\n",
    "df = spark.read.format(\"csv\").load(\"/home/jovyan/work/data/zipcodes.csv\")\n",
    "\n",
    "# Use header record for column names\n",
    "df2 = spark.read.option(\"header\",True).csv(\"/home/jovyan/work/data/zipcodes.csv\")\n",
    "\n",
    "#PySpark reads all columns as a string (StringType) by default.\n",
    "\n",
    "# Using delimiter option\n",
    "df3 = spark.read.options(delimiter=',').csv(\"/home/jovyan/work/data/zipcodes.csv\")\n",
    "\n",
    "# Using inferschema and delimiter - The default value set to this option is False when setting to true it automatically infers column types \n",
    "#based on the data\n",
    "df4 = spark.read.options(inferSchema='True',delimiter=',').csv(\"/home/jovyan/work/data/zipcodes.csv\")\n",
    "\n",
    "# Define read options\n",
    "options = {\n",
    "    \"inferSchema\": \"True\",\n",
    "    \"delimiter\": \",\"\n",
    "}\n",
    "\n",
    "# Read a CSV file with specified options\n",
    "df4 = spark.read.options(**options).csv(\"/home/jovyan/work/data/zipcodes.csv\")\n",
    "\n",
    "# Chaining multiple options\n",
    "df4 = spark.read.option(\"inferSchema\",True).option(\"delimiter\",\",\") .csv(\"/home/jovyan/work/data/zipcodes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f7392da-d8ce-4c3a-83cd-e85195d23e3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RecordNumber: integer (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- ZipCodeType: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- LocationType: string (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Xaxis: integer (nullable = true)\n",
      " |-- Yaxis: double (nullable = true)\n",
      " |-- Zaxis: double (nullable = true)\n",
      " |-- WorldRegion: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- LocationText: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Decommisioned: boolean (nullable = true)\n",
      " |-- TaxReturnsFiled: string (nullable = true)\n",
      " |-- EstimatedPopulation: integer (nullable = true)\n",
      " |-- TotalWages: integer (nullable = true)\n",
      " |-- Notes: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType \n",
    "from pyspark.sql.types import ArrayType, DoubleType, BooleanType\n",
    "\n",
    "# Using custom schema\n",
    "schema = StructType() \\\n",
    "      .add(\"RecordNumber\",IntegerType(),True) \\\n",
    "      .add(\"Zipcode\",IntegerType(),True) \\\n",
    "      .add(\"ZipCodeType\",StringType(),True) \\\n",
    "      .add(\"City\",StringType(),True) \\\n",
    "      .add(\"State\",StringType(),True) \\\n",
    "      .add(\"LocationType\",StringType(),True) \\\n",
    "      .add(\"Lat\",DoubleType(),True) \\\n",
    "      .add(\"Long\",DoubleType(),True) \\\n",
    "      .add(\"Xaxis\",IntegerType(),True) \\\n",
    "      .add(\"Yaxis\",DoubleType(),True) \\\n",
    "      .add(\"Zaxis\",DoubleType(),True) \\\n",
    "      .add(\"WorldRegion\",StringType(),True) \\\n",
    "      .add(\"Country\",StringType(),True) \\\n",
    "      .add(\"LocationText\",StringType(),True) \\\n",
    "      .add(\"Location\",StringType(),True) \\\n",
    "      .add(\"Decommisioned\",BooleanType(),True) \\\n",
    "      .add(\"TaxReturnsFiled\",StringType(),True) \\\n",
    "      .add(\"EstimatedPopulation\",IntegerType(),True) \\\n",
    "      .add(\"TotalWages\",IntegerType(),True) \\\n",
    "      .add(\"Notes\",StringType(),True)\n",
    "      \n",
    "df_with_schema = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", True) \\\n",
    "      .schema(schema) \\\n",
    "      .load(\"/home/jovyan/work/data/zipcodes.csv\")\n",
    "\n",
    "df_with_schema.printSchema()\n",
    "\n",
    "#to include header writing to file\n",
    "df2.write.option(\"header\",True) \\\n",
    " .csv(\"/home/jovyan/work/data/raw/spark_output/zipcodes123\")\n",
    "\n",
    "#Saving modes\n",
    "#overwrite – Overwrite the existing file if already exists.\n",
    "#append – New rows are appended to the existing rows.\n",
    "#ignore – When this option is used, it ignores the writing operation when the file already exists.\n",
    "#error – This option returns an error when the file already exists. This is a default option.\n",
    "df2.write.mode('overwrite').csv(\"/tmp/spark_output/zipcodes\")\n",
    "\n",
    "# You can also use this\n",
    "df2.write.format(\"csv\").mode('overwrite').save(\"/home/jovyan/work/data/raw/spark_output/zipcodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25489244-fde6-42f9-95c2-f62b33183cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Commonly used options:\n",
    "header: Specifies whether to include a header row with column names in the CSV file. Example: option(\"header\", \"true\").\n",
    "delimiter: Specifies the delimiter to use between fields in the CSV file. Example: option(\"delimiter\", \",\").\n",
    "quote: Specifies the character used for quoting fields in the CSV file. Example: option(\"quote\", \"\\\"\").\n",
    "escape: Specifies the escape character used in the CSV file. Example: option(\"escape\", \"\\\\\").\n",
    "nullValue: Specifies the string to represent null values in the CSV file. Example: option(\"nullValue\", \"NA\").\n",
    "dateFormat: Specifies the date format to use for date columns. Example: option(\"dateFormat\", \"yyyy-MM-dd\").\n",
    "mode: Specifies the write mode for the output. Options include “overwrite”, “append”, “ignore”, and “error”. Example: option(\"mode\", \"overwrite\").\n",
    "compression: Specifies the compression codec to use for the output file. Example: option(\"compression\", \"gzip\")."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
