{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c18c22e-84b8-4f0e-ad4e-a72ca1676218",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Broadcast variable - create copy of dimension table on each worker node and performs operations - so no shuffle is needed and \n",
    "performance is better - cached in each node of cluster.ideal for joining tables where one of it is tiny tables .\n",
    "Only suitable for smaller tables.its sent to worker thru driver.so need to fit into driver memory\n",
    "With no broadcast - data is shuffled across all worker nodes for any transformations/ actions and permanent hits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Broadcast variables (read-only shared variable)\n",
    "\n",
    "\n",
    "Broadcast variables are shared, read-only variables cached and accessible across all nodes in a cluster for use by tasks. \n",
    "Rather than transmitting this data with every task, PySpark employs efficient broadcast algorithms to distribute broadcast variables\n",
    "to machines, thereby reducing communication costs.broadcast variables are not sent to executors with sc.Broadcast(variable) call instead,\n",
    "they will be sent to executors when they are first used.\n",
    "\n",
    "Accumulator variables (updatable shared variables)\n",
    "Accumulators are write-only and initialize once variables where only tasks that are running on workers are allowed to update and updates\n",
    "from the workers get propagated automatically to the driver program. But, only the driver program is allowed to access the Accumulator \n",
    "variable using the value property.These variables are shared by all executors to update and add information through aggregation or \n",
    "computative operations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create broadcast variable\n",
    "broadcastVar = sc.broadcast([0, 1, 2, 3])\n",
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accumulator variables are created using SparkContext.longAccumulator(v)\n",
    "# Create accumulator variable\n",
    "accum = sc.longAccumulator(\"SumAccumulator\")\n",
    "sc.parallelize([1, 2, 3]).foreach(lambda x: accum.add(x))\n",
    "#rdd.foreach() is executed on workers and accum.value is called from PySpark driver program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc3441cd-de75-4b73-b96a-3e301b7b6b7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Store_id</th><th>Item</th><th>Amount</th></tr></thead><tbody><tr><td>100</td><td>Cosmetic</td><td>150</td></tr><tr><td>200</td><td>Apparel</td><td>200</td></tr><tr><td>300</td><td>Shirt</td><td>300</td></tr><tr><td>400</td><td>Trouser</td><td>400</td></tr><tr><td>500</td><td>Socks</td><td>200</td></tr><tr><td>100</td><td>Belt</td><td>70</td></tr><tr><td>200</td><td>Cosmetic</td><td>200</td></tr><tr><td>300</td><td>Shoe</td><td>70</td></tr><tr><td>400</td><td>Socks</td><td>40</td></tr><tr><td>500</td><td>Shorts</td><td>100</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         100,
         "Cosmetic",
         150
        ],
        [
         200,
         "Apparel",
         200
        ],
        [
         300,
         "Shirt",
         300
        ],
        [
         400,
         "Trouser",
         400
        ],
        [
         500,
         "Socks",
         200
        ],
        [
         100,
         "Belt",
         70
        ],
        [
         200,
         "Cosmetic",
         200
        ],
        [
         300,
         "Shoe",
         70
        ],
        [
         400,
         "Socks",
         40
        ],
        [
         500,
         "Shorts",
         100
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Store_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Item",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Amount",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Transaction = [\n",
    "    (100,'Cosmetic',150),\n",
    "    (200,'Apparel',200),\n",
    "    (300,'Shirt',300),\n",
    "    (400,'Trouser',400),\n",
    "    (500,'Socks',200),\n",
    "    (100,'Belt',70),\n",
    "    (200,'Cosmetic',200),\n",
    "    (300,'Shoe',70),\n",
    "    (400,'Socks',40),\n",
    "    (500,'Shorts',100)\n",
    "]\n",
    "\n",
    "transaction_DF = spark.createDataFrame(data=Transaction,schema=['Store_id','Item','Amount'])\n",
    "display(transaction_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d7a87f0-eaf5-47de-8a8e-514a21ed85d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Store_id</th><th>Store_name</th></tr></thead><tbody><tr><td>100</td><td>Store_London</td></tr><tr><td>200</td><td>Store_Bangkok</td></tr><tr><td>300</td><td>Store_UK</td></tr><tr><td>400</td><td>Store_Amer</td></tr><tr><td>500</td><td>Store_India</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         100,
         "Store_London"
        ],
        [
         200,
         "Store_Bangkok"
        ],
        [
         300,
         "Store_UK"
        ],
        [
         400,
         "Store_Amer"
        ],
        [
         500,
         "Store_India"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Store_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Store_name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Store = [\n",
    "    (100,'Store_London'),\n",
    "    (200,'Store_Bangkok'),\n",
    "    (300,'Store_UK'),\n",
    "    (400,'Store_Amer'),\n",
    "    (500,'Store_India')\n",
    "]\n",
    "\n",
    "store_DF = spark.createDataFrame(data= Store,schema=['Store_id','Store_name'])\n",
    "display(store_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d06f0e5-fac4-4955-ad29-01912b111896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Store_id</th><th>Item</th><th>Amount</th><th>Store_id</th><th>Store_name</th></tr></thead><tbody><tr><td>100</td><td>Cosmetic</td><td>150</td><td>100</td><td>Store_London</td></tr><tr><td>200</td><td>Apparel</td><td>200</td><td>200</td><td>Store_Bangkok</td></tr><tr><td>300</td><td>Shirt</td><td>300</td><td>300</td><td>Store_UK</td></tr><tr><td>400</td><td>Trouser</td><td>400</td><td>400</td><td>Store_Amer</td></tr><tr><td>500</td><td>Socks</td><td>200</td><td>500</td><td>Store_India</td></tr><tr><td>100</td><td>Belt</td><td>70</td><td>100</td><td>Store_London</td></tr><tr><td>200</td><td>Cosmetic</td><td>200</td><td>200</td><td>Store_Bangkok</td></tr><tr><td>300</td><td>Shoe</td><td>70</td><td>300</td><td>Store_UK</td></tr><tr><td>400</td><td>Socks</td><td>40</td><td>400</td><td>Store_Amer</td></tr><tr><td>500</td><td>Shorts</td><td>100</td><td>500</td><td>Store_India</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         100,
         "Cosmetic",
         150,
         100,
         "Store_London"
        ],
        [
         200,
         "Apparel",
         200,
         200,
         "Store_Bangkok"
        ],
        [
         300,
         "Shirt",
         300,
         300,
         "Store_UK"
        ],
        [
         400,
         "Trouser",
         400,
         400,
         "Store_Amer"
        ],
        [
         500,
         "Socks",
         200,
         500,
         "Store_India"
        ],
        [
         100,
         "Belt",
         70,
         100,
         "Store_London"
        ],
        [
         200,
         "Cosmetic",
         200,
         200,
         "Store_Bangkok"
        ],
        [
         300,
         "Shoe",
         70,
         300,
         "Store_UK"
        ],
        [
         400,
         "Socks",
         40,
         400,
         "Store_Amer"
        ],
        [
         500,
         "Shorts",
         100,
         500,
         "Store_India"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Store_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Item",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Amount",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Store_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "Store_name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "join_DF = transaction_DF.join(broadcast(store_DF),transaction_DF['Store_id']== store_DF['Store_id'])\n",
    "display(join_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a5305b3-c966-4b5b-a15e-6a88345c1ade",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Join Inner, (Store_id#36L = Store_id#15L)\n",
      ":- LogicalRDD [Store_id#36L, Item#37, Amount#38L], false\n",
      "+- ResolvedHint (strategy=broadcast)\n",
      "   +- LogicalRDD [Store_id#15L, Store_name#16], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "Store_id: bigint, Item: string, Amount: bigint, Store_id: bigint, Store_name: string\n",
      "Join Inner, (Store_id#36L = Store_id#15L)\n",
      ":- LogicalRDD [Store_id#36L, Item#37, Amount#38L], false\n",
      "+- ResolvedHint (strategy=broadcast)\n",
      "   +- LogicalRDD [Store_id#15L, Store_name#16], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join Inner, (Store_id#36L = Store_id#15L), rightHint=(strategy=broadcast)\n",
      ":- Filter isnotnull(Store_id#36L)\n",
      ":  +- LogicalRDD [Store_id#36L, Item#37, Amount#38L], false\n",
      "+- Filter isnotnull(Store_id#15L)\n",
      "   +- LogicalRDD [Store_id#15L, Store_name#16], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=true\n",
      "+- == Final Plan ==\n",
      "   *(2) BroadcastHashJoin [Store_id#36L], [Store_id#15L], Inner, BuildRight, false, true\n",
      "   :- *(2) Filter isnotnull(Store_id#36L)\n",
      "   :  +- *(2) Scan ExistingRDD[Store_id#36L,Item#37,Amount#38L]\n",
      "   +- ShuffleQueryStage 0, Statistics(sizeInBytes=192.0 B, rowCount=5, isRuntime=true)\n",
      "      +- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=213]\n",
      "         +- *(1) Filter isnotnull(Store_id#15L)\n",
      "            +- *(1) Scan ExistingRDD[Store_id#15L,Store_name#16]\n",
      "+- == Initial Plan ==\n",
      "   BroadcastHashJoin [Store_id#36L], [Store_id#15L], Inner, BuildRight, false, true\n",
      "   :- Filter isnotnull(Store_id#36L)\n",
      "   :  +- Scan ExistingRDD[Store_id#36L,Item#37,Amount#38L]\n",
      "   +- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=178]\n",
      "      +- Filter isnotnull(Store_id#15L)\n",
      "         +- Scan ExistingRDD[Store_id#15L,Store_name#16]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_DF.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('James', 'Smith', 'USA', 'California'), ('Michael', 'Rose', 'USA', 'New York'), ('Robert', 'Williams', 'USA', 'California'), ('Maria', 'Jones', 'USA', 'Florida')]\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n",
      "+---------+--------+-------+-----+\n",
      "|firstname|lastname|country|state|\n",
      "+---------+--------+-------+-----+\n",
      "|James    |Smith   |USA    |CA   |\n",
      "|Michael  |Rose    |USA    |NY   |\n",
      "|Robert   |Williams|USA    |CA   |\n",
      "|Maria    |Jones   |USA    |FL   |\n",
      "+---------+--------+-------+-----+\n",
      "\n",
      "+---------+--------+-------+----------+\n",
      "|firstname|lastname|country|state     |\n",
      "+---------+--------+-------+----------+\n",
      "|James    |Smith   |USA    |California|\n",
      "|Michael  |Rose    |USA    |New York  |\n",
      "|Robert   |Williams|USA    |California|\n",
      "|Maria    |Jones   |USA    |Florida   |\n",
      "+---------+--------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = df.rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).toDF(columns)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.sql.functions.lit.\n: org.apache.spark.SparkRuntimeException: The feature is not supported: literal for '{FL=Florida, NY=New York, CA=California}' of class java.util.HashMap.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.literalTypeUnsupportedError(QueryExecutionErrors.scala:296)\n\tat org.apache.spark.sql.catalyst.expressions.Literal$.apply(literals.scala:101)\n\tat org.apache.spark.sql.functions$.lit(functions.scala:125)\n\tat org.apache.spark.sql.functions.lit(functions.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Broadcast variable on filter\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m filteDf\u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwhere((\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbroadcastStates\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/column.py:725\u001b[0m, in \u001b[0;36mColumn.isin\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cols[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mset\u001b[39m)):\n\u001b[1;32m    722\u001b[0m     cols \u001b[38;5;241m=\u001b[39m cast(Tuple, cols[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    723\u001b[0m cols \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m    724\u001b[0m     Tuple,\n\u001b[0;32m--> 725\u001b[0m     [c\u001b[38;5;241m.\u001b[39m_jc \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, Column) \u001b[38;5;28;01melse\u001b[39;00m _create_column_from_literal(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols],\n\u001b[1;32m    726\u001b[0m )\n\u001b[1;32m    727\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/column.py:725\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cols[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mset\u001b[39m)):\n\u001b[1;32m    722\u001b[0m     cols \u001b[38;5;241m=\u001b[39m cast(Tuple, cols[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    723\u001b[0m cols \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m    724\u001b[0m     Tuple,\n\u001b[0;32m--> 725\u001b[0m     [c\u001b[38;5;241m.\u001b[39m_jc \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, Column) \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_create_column_from_literal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols],\n\u001b[1;32m    726\u001b[0m )\n\u001b[1;32m    727\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/column.py:50\u001b[0m, in \u001b[0;36m_create_column_from_literal\u001b[0;34m(literal)\u001b[0m\n\u001b[1;32m     48\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mliteral\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.sql.functions.lit.\n: org.apache.spark.SparkRuntimeException: The feature is not supported: literal for '{FL=Florida, NY=New York, CA=California}' of class java.util.HashMap.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.literalTypeUnsupportedError(QueryExecutionErrors.scala:296)\n\tat org.apache.spark.sql.catalyst.expressions.Literal$.apply(literals.scala:101)\n\tat org.apache.spark.sql.functions$.lit(functions.scala:125)\n\tat org.apache.spark.sql.functions.lit(functions.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "# Broadcast variable on filter\n",
    "filteDf= df.where((df['state'].isin(broadcastStates.value)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "15\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"accumulator\").getOrCreate()\n",
    "\n",
    "accum=spark.sparkContext.accumulator(0)\n",
    "rdd=spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "rdd.foreach(lambda x:accum.add(x))\n",
    "print(accum.value)\n",
    "\n",
    "accuSum=spark.sparkContext.accumulator(0)\n",
    "def countFun(x):\n",
    "    global accuSum\n",
    "    accuSum+=x\n",
    "rdd.foreach(countFun)\n",
    "print(accuSum.value)\n",
    "\n",
    "accumCount=spark.sparkContext.accumulator(0)\n",
    "rdd2=spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "rdd2.foreach(lambda x:accumCount.add(1))\n",
    "print(accumCount.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Broadcast Variable",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
